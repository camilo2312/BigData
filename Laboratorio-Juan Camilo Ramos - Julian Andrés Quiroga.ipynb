{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8d22f8-a9e1-4d41-85bc-88ac380ba8d0",
   "metadata": {},
   "source": [
    "Ejercicios\n",
    "Julian Andrés Quiroga\n",
    "Juan Camilo Ramos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff906ca-a5f4-4db4-bb4a-a9c009efbfd0",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "\n",
    "Give an example of a question we might be able to answer with this sort of data, and another question that we'd need additional data to answer. Assume for now\n",
    "that all of the reviews are coming from one business.\n",
    "\n",
    "Question 1: Solved with data analysis:\n",
    "\n",
    "What is the relationship between the number of stars awarded and the number of \"helpful\", \"funny\" or \"great\" votes the reviews receive?\n",
    "\n",
    "Question 2: Solved with aditional data\n",
    "\n",
    "What are the key factors that influence a customer rating a restaurant 5 stars instead of 3 or less?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df3eb1-e06a-4d6c-afd1-6a7e882ac964",
   "metadata": {},
   "source": [
    "Exercise 2\n",
    "\n",
    "Conduct an exploratory analysis of the sizes of reviews: find the shortest and longest reviews, then plot a histogram showing the distribution of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396933b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carga el archivo csv\n",
    "data = pd.read_csv('sdata.csv')\n",
    "data.head()\n",
    "\n",
    "# Extrae solo la columna texto\n",
    "AllReviews = data['text']\n",
    "AllReviews.head()\n",
    "\n",
    "# Calcular el tamaño de las reseñas en términos de número de palabras\n",
    "# utilizando la función `str.split()` para dividir las palabras, y luego usamos `len()` para contar las palabras\n",
    "tamanios_resenas = AllReviews.apply(lambda x: len(x.split()))\n",
    "\n",
    "# Encontrar la reseña más corta y más larga\n",
    "resena_larga = AllReviews[tamanios_resenas.idxmax()]\n",
    "resena_corta = AllReviews[tamanios_resenas.idxmin()]\n",
    "\n",
    "# Diagrama de barras con la información calculada anteriormente\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(tamanios_resenas, bins=30, edgecolor='black')\n",
    "plt.title('Tamaño de las reseñas en funión del número de palabras')\n",
    "plt.xlabel('Número de palabras')\n",
    "plt.ylabel('Cantidad')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2c490-3d52-42de-9bbc-a76607117f3f",
   "metadata": {},
   "source": [
    "Exercise 3\n",
    "\n",
    "Write a function word_cloud_rating(data, star_value) that constructs a word cloud from the subset of data that exhibit a certain star_value. Visualize the results of\n",
    "this function for 1-star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6368c-a688-4424-a654-0603b7d15cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def word_cloud_rating(datos, valor_estrella):\n",
    "    # Obtiene las reseñas que son iguales al número de estrellas\n",
    "    reseñas_filtradas = datos[datos['stars'] == valor_estrella]['text']\n",
    "    \n",
    "    # Se unen todas en un solo texto\n",
    "    texto = \" \".join(reseñas_filtradas)\n",
    "    \n",
    "    # Se crea una lista de stopwrod para mostrar la nube de palabras\n",
    "    stopwords = set(STOPWORDS)\n",
    "    \n",
    "    # Crear la nube de palabras\n",
    "    nube_palabras = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(texto)\n",
    "    \n",
    "    # Visualizar la nube\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(nube_palabras, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1863d-266e-402d-956a-b3e45ade8d76",
   "metadata": {},
   "source": [
    "Excercise 4\n",
    "\n",
    "The word \"good\" seems to appear quite frequently in the negative reviews. Investigate why that is and come up with a reasonable explanation.\n",
    "\n",
    "Anwser\n",
    "\n",
    "The use of the word \"good\" in negative reviews is often a strategy for softening criticism. People use it to tone down their feedback, be more diplomatic, and avoid coming across as excessively harsh. By highlighting a positive aspect, even the smallest one, they seek to balance their critique, offering a more fair and reasonable opinion. This also reflects the need to maintain courtesy, avoid confrontation, and make their feedback appear constructive rather than purely negative. In this way, \"bueno\" acts as a buffer to express dissatisfaction without being aggressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bec19-b239-481e-888d-994beaed78bd",
   "metadata": {},
   "source": [
    "Excercise 5\n",
    "\n",
    "Find all the high-frequency (top 1%) and low-frequency (bottom 1%) words in the reviews overall. (Hint: import the Counter() function from the collections class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae3510-a391-4d52-ae1b-e0cae9feb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Descargar los recursos de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar los datos (sdata.csv)\n",
    "data = pd.read_csv('sdata.csv')\n",
    "\n",
    "# Filtrar la columna de texto\n",
    "AllReviews = data['text']\n",
    "\n",
    "# Paso 1: Tokenizar todas las reseñas y eliminamos las stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Combinar todas las reseñas en un solo bloque de texto\n",
    "all_text = ' '.join(AllReviews.astype(str))\n",
    "\n",
    "# Tokenizar en palabras y eliminar las stopwords\n",
    "words = nltk.word_tokenize(all_text.lower())\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Paso 2: Contar la frecuencia de cada palabra\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Total de palabras\n",
    "total_words = sum(word_counts.values())\n",
    "\n",
    "# Paso 3: Encontrar el 1% más alto y el 1% más bajo\n",
    "# Ordenar las palabras por frecuencia\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Calcular el umbral del 1%\n",
    "top_1_percent_index = int(len(sorted_word_counts) * 0.01)\n",
    "low_1_percent_index = int(len(sorted_word_counts) * 0.99)\n",
    "\n",
    "# Paso 4: Extraer palabras de alta y baja frecuencia\n",
    "top_1_percent_words = sorted_word_counts[:top_1_percent_index]\n",
    "low_1_percent_words = sorted_word_counts[low_1_percent_index:]\n",
    "\n",
    "# Imprimir las palabras de alta y baja frecuencia\n",
    "print(\"Palabras de alta frecuencia (1% más alto):\")\n",
    "for word, count in top_1_percent_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nPalabras de baja frecuencia (1% más bajo):\")\n",
    "for word, count in low_1_percent_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea9896-635e-4a2a-940a-3f6118d556f7",
   "metadata": {},
   "source": [
    "Excercise 6\n",
    "\n",
    "Write a function called top_k_ngrams(word_tokens, n, k) for printing out the top k n-grams. Use this function to get the top 10 1-grams, 2-grams, and 3-grams from\n",
    "the first 1000 reviews in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859881c3-16a8-4d26-92d9-cbac8ab236f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar los recursos de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar los datos (sdata.csv) y obtener las primeras 1000 revisiones\n",
    "data = pd.read_csv('sdata.csv', nrows=1000)\n",
    "\n",
    "# Extraer las primeras 1000 reseñas\n",
    "AllReviews = data['text']\n",
    "\n",
    "# Tokenizar y limpiar las reseñas (eliminar stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "all_text = ' '.join(AllReviews.astype(str))\n",
    "words = nltk.word_tokenize(all_text.lower())\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Función para obtener los k n-gramas superiores\n",
    "def top_k_ngrams(word_tokens, n, k):\n",
    "    # Convertir los tokens en un solo texto\n",
    "    word_text = ' '.join(word_tokens)\n",
    "    \n",
    "    # Usar CountVectorizer para contar los n-gramas\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit([word_text])\n",
    "    bag_of_words = vec.transform([word_text])\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    # Obtener los n-gramas y sus frecuencias\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Imprimir los k n-gramas más frecuentes\n",
    "    print(f\"Top {k} {n}-grams:\")\n",
    "    for word, freq in words_freq[:k]:\n",
    "        print(f\"{word}: {freq}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Aplicar la función para obtener los 10 mejores 1-gramas, 2-gramas y 3-gramas\n",
    "top_k_ngrams(filtered_words, 1, 10)  # 1-gramas\n",
    "top_k_ngrams(filtered_words, 2, 10)  # 2-gramas\n",
    "top_k_ngrams(filtered_words, 3, 10)  # 3-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13821788-d9af-4406-a2ec-7397a9cd211d",
   "metadata": {},
   "source": [
    "Excercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97c03b-98cc-4989-a95a-add5c06db077",
   "metadata": {},
   "source": [
    "7.1\n",
    "Filter out all of the stop words in the first review of the Yelp review data and print out your answer. Additionally, print out (separately) the stopwords you found in\n",
    "this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8d3b0-2f82-47b0-b8f6-cfd747ae4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar los datos (sdata.csv)\n",
    "data = pd.read_csv('sdata.csv', nrows=5000)\n",
    "\n",
    "# Obtener la primera revisión\n",
    "first_review = data['text'][0]\n",
    "\n",
    "# Tokenizar la primera revisión en palabras individuales\n",
    "words_in_review = nltk.word_tokenize(first_review.lower())\n",
    "\n",
    "# Obtener las palabras vacías en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filtrar palabras vacías y no vacías\n",
    "stopwords_in_review = [word for word in words_in_review if word in stop_words]\n",
    "filtered_review = [word for word in words_in_review if word not in stop_words]\n",
    "\n",
    "# Imprimir las palabras vacías encontradas\n",
    "print(\"Palabras vacías encontradas en la primera revisión:\")\n",
    "print(stopwords_in_review)\n",
    "\n",
    "# Imprimir la revisión sin las palabras vacías\n",
    "print(\"\\nPrimera revisión sin palabras vacías:\")\n",
    "print(' '.join(filtered_review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf1048-10da-473e-8061-74f8b8ca6e10",
   "metadata": {},
   "source": [
    "7.2\n",
    "Modify the function top_k_ngrams(word_tokens, n, k) to remove stop words before determining the top n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6cc06-1914-4add-a172-d348c749193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar los recursos de NLTK (si no están descargados ya)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Obtener las stopwords en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función modificada para obtener los k n-gramas superiores sin palabras vacías\n",
    "def top_k_ngrams(word_tokens, n, k):\n",
    "    # Filtrar las palabras vacías de los tokens\n",
    "    filtered_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # Transformar los tokens filtrados en un solo texto\n",
    "    word_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # Usar CountVectorizer para contar los n-gramas de las palabras vacías\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit([word_text])\n",
    "    bag_of_words = vec.transform([word_text])\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    # Obtener los n-gramas y sus frecuencias\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Imprimir por consola los k n-gramas más frecuentes\n",
    "    print(f\"Top {k} {n}-grams sin palabras vacías:\")\n",
    "    for word, freq in words_freq[:k]:\n",
    "        print(f\"{word}: {freq}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Ejemplo con 1000 datos de cómo usar la función con las primeras 1000 revisiones\n",
    "data = pd.read_csv('sdata.csv', nrows=1000)\n",
    "AllReviews = data['text']\n",
    "\n",
    "# Tokenización y preparación de palabras\n",
    "all_text = ' '.join(AllReviews.astype(str))\n",
    "words = nltk.word_tokenize(all_text.lower())\n",
    "\n",
    "# Obtener los 10 mejores 1-gramas, 2-gramas y 3-gramas, eliminando palabras vacías\n",
    "top_k_ngrams(words, 1, 10)  # 1-gramas\n",
    "top_k_ngrams(words, 2, 10)  # 2-gramas\n",
    "top_k_ngrams(words, 3, 10)  # 3-gramas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad60a2f-b7b7-4b12-a08a-46fa0bb4ff86",
   "metadata": {},
   "source": [
    "Excercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bf0f5-efb5-413a-9ed6-d531a6737b8e",
   "metadata": {},
   "source": [
    "8.1\n",
    "\n",
    "Divide the data into \"good reviews\" (i.e. stars rating was greater than 3) and \"bad reviews\" (i.e. stars rating was less or equal than 3) and make a bar plot of the top\n",
    "20 words in each case. Are these results different from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c5758-be66-43ae-87e9-7912b55b071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('sdata.csv', nrows=5000)\n",
    "\n",
    "# Filtrar palabras vacías\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "# Función para obtener las palabras más frecuentes\n",
    "def get_top_n_words(corpus, n=20):\n",
    "    vec = CountVectorizer(stop_words=stop_words).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Dividir las críticas en buneas y malas buenas (>3 estrellas) y malas (<=3 estrellas)\n",
    "good_reviews = data[data['stars'] > 3]['text']\n",
    "bad_reviews = data[data['stars'] <= 3]['text']\n",
    "\n",
    "# Obtener las 20 palabras más frecuentes en \"buenas críticas\"\n",
    "top_good_words = get_top_n_words(good_reviews, 20)\n",
    "\n",
    "# Obtener las 20 palabras más frecuentes en \"malas críticas\"\n",
    "top_bad_words = get_top_n_words(bad_reviews, 20)\n",
    "\n",
    "# Crear DataFrames para cada conjunto de palabras\n",
    "df_good = pd.DataFrame(top_good_words, columns=['Word', 'Frequency'])\n",
    "df_bad = pd.DataFrame(top_bad_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# Grafica de las mejores 20 palabras\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_good.groupby('Word').sum()['Frequency'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 palabras \"Buenas Criticas\"')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Grafica de las peores 20 palabras\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_bad.groupby('Word').sum()['Frequency'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 palarbas \"Malas criticas\"')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d04c7-d56d-4f0a-93ee-c090cd0e4edd",
   "metadata": {},
   "source": [
    "La diferencia entre las palabras más comunes en reseñas \"buenas\" y \"malas\" podría ser muy relevante en la analítica de datos significativa. Por ejemplo, las \"buenas críticas\" podrían tener más palabras asociadas con satisfacción, buenas vibras, elecciones éxitosas, gustos, etc; mientras que las \"malas críticas\" tendrían más palabras negativas, como podrían ser no me gusta, amargo, disgustos, etc. Este análisis permitirá ver claramente cómo el sentimiento de las reseñas afecta la frecuencia de ciertas palabras en cada categoría."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e4b66-842c-4a6a-ac9d-bb7ad897d900",
   "metadata": {},
   "source": [
    "8.2\n",
    "\n",
    "Use the get_top_n_words() function to find the top 20 bigrams and trigrams (In both, bad and good reviews). Do the results seem useful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf923ae8-a31f-4f86-b17b-3fa79f14a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('sdata.csv', nrows=5000)\n",
    "\n",
    "# Convertir el conjunto de palabras vacías en una lista\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "# Función para obtener los n-gramas más frecuentes\n",
    "def get_top_n_words(corpus, n=20, ngram_range=(1, 1)):\n",
    "    # Ajustar el rango de n-gramas (bigrama, trigrama, etc.)\n",
    "    vec = CountVectorizer(stop_words=stop_words, ngram_range=ngram_range).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Dividir las críticas en \"buenas\" (>3 estrellas) y \"malas\" (<=3 estrellas)\n",
    "good_reviews = data[data['stars'] > 3]['text']\n",
    "bad_reviews = data[data['stars'] <= 3]['text']\n",
    "\n",
    "# Obtener los 20 bigramas más frecuentes en \"buenas críticas\"\n",
    "top_good_bigrams = get_top_n_words(good_reviews, 20, ngram_range=(2, 2))\n",
    "\n",
    "# Obtener los 20 bigramas más frecuentes en \"malas críticas\"\n",
    "top_bad_bigrams = get_top_n_words(bad_reviews, 20, ngram_range=(2, 2))\n",
    "\n",
    "# Obtener los 20 trigramas más frecuentes en \"buenas críticas\"\n",
    "top_good_trigrams = get_top_n_words(good_reviews, 20, ngram_range=(3, 3))\n",
    "\n",
    "# Obtener los 20 trigramas más frecuentes en \"malas críticas\"\n",
    "top_bad_trigrams = get_top_n_words(bad_reviews, 20, ngram_range=(3, 3))\n",
    "\n",
    "# Crear DataFrames para cada conjunto de n-gramas\n",
    "df_good_bigrams = pd.DataFrame(top_good_bigrams, columns=['Bigram', 'Frequency'])\n",
    "df_bad_bigrams = pd.DataFrame(top_bad_bigrams, columns=['Bigram', 'Frequency'])\n",
    "df_good_trigrams = pd.DataFrame(top_good_trigrams, columns=['Trigram', 'Frequency'])\n",
    "df_bad_trigrams = pd.DataFrame(top_bad_trigrams, columns=['Trigram', 'Frequency'])\n",
    "\n",
    "# Gráfica top de 20 bigramas en buenas críticas\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_good_bigrams.groupby('Bigram').sum()['Frequency'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 Bigramas \"Buenas críticas\"')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Gráfica top de 20 bigramas en malas críticas\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_bad_bigrams.groupby('Bigram').sum()['Frequency'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 Bigramas \"Malas críticas\"')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Gráfica top de 20 trigramas en buenas críticas\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_good_trigrams.groupby('Trigram').sum()['Frequency'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 Trigramas \"Buenas críticas\"')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Gráfica top de 20 trigramas en malas críticas\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_bad_trigrams.groupby('Trigram').sum()['Frequency'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 Trigramas \"Malas críticas\"')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad2eeb",
   "metadata": {},
   "source": [
    "¿Son los resultados útiles?\n",
    "Bigramas y trigramas:\n",
    "Los bigramas y trigramas pueden proporcionar contexto adicional al mostrar combinaciones comunes de palabras que aparecen juntas, \n",
    "como \"great service\" o \"bad experience\".\n",
    "\n",
    "En buenas críticas, es probable que encuentres combinaciones positivas como \"friendly staff\" o \"delicious food\".\n",
    "\n",
    "En malas críticas, podrías encontrar bigramas o trigramas negativos como \"bad service\" o \"wait long time\".\n",
    "\n",
    "Al comparar estos resultados con las palabras individuales más comunes, los bigramas y trigramas ofrecen una \n",
    "visión más detallada del contexto y pueden ayudar a detectar frases recurrentes tanto en las experiencias \n",
    "positivas como en las negativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e58456-9469-479f-bb42-f7d24e870902",
   "metadata": {},
   "source": [
    "Excercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bf2f9-f214-4074-b8bc-e6b5f529255f",
   "metadata": {},
   "source": [
    "9.1\n",
    "\n",
    "You may have noticed that many of the important \"bad\" bigrams included the words \"like\" or \"just\" but didn't seem very informative (e.g. \"felt like\", \"food just\").\n",
    "Give some ideas of how to use this sort of observation in future pre-processing of reviews, based on the pre-processing ideas we have already studied.\n",
    "\n",
    "Anwser\n",
    "\n",
    "Based on the observations of \"bad\" bigrams such as \"like\" and \"just,\" which seem uninformative in sentiment analysis, there are several preprocessing strategies we can apply to improve the quality of the data:\n",
    "\n",
    "1. Stopword Removal or Modification: Words like \"like\" and \"just\" may function as stopwords, adding little meaning in most contexts. Since they frequently appear in non-informative bigrams, it might be useful to either remove them during preprocessing or handle them differently depending on the context. For example, removing them when they appear in bigrams like \"felt like\" or \"just food\" could reduce noise and improve model performance.\n",
    "\n",
    "2. Bigrams as Features: In future analyses, we could flag bigrams that contain these non-informative words as low-value features. By reducing their weight or excluding them from the model, we would be focusing more on phrases that convey significant sentiment or opinions.\n",
    "\n",
    "3. POS Tagging and Filtering: These words often act as fillers rather than carrying semantic weight. By using Part-of-Speech (POS) tagging, we could filter out filler words and reduce the overall feature space, helping the model focus on more informative content.\n",
    "\n",
    "4. Custom Word Lists: Instead of globally removing these words, we could build a custom list of terms that often form weak bigrams, like \"felt like\" or \"just food,\" and exclude or modify them during preprocessing. This approach allows us to retain some instances of these words when they contribute meaning but eliminate them when they detract from analysis quality.\n",
    "\n",
    "These strategies aim to filter out uninformative patterns that could dilute the insights gained from review data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a2e2a-eb40-4aee-baa9-147a5a961d9e",
   "metadata": {},
   "source": [
    "9.2\n",
    "\n",
    "Building on the previous question, we note that most of the most important complaints and compliments can't be completely observed by looking at bigrams or\n",
    "trigrams. This can often be fixed by small modifications. Do the following:\n",
    "1. Write down a complaint that is unlikely to be (completely) picked up by bigram analysis. Hint: what might you write if your hamburger was served cold?\n",
    "2. Write down a processing step that would fix this problem. Try to find a solution that would work for several similar problems without additional human\n",
    "input.\n",
    "\n",
    "\n",
    "Anwser\n",
    "\n",
    "1. Complaint Example: \"I was so disappointed because my hamburger was served cold and it took forever for the server to bring it back hot.\"\n",
    "\n",
    "2. Processing Step: One effective solution could be to implement dependency parsing to capture relationships between words and understand the context more accurately. This would help identify that \"hamburger\" is the subject, \"served\" is the action, and \"cold\" is the key complaint, even if these words aren't in adjacent positions (which bigram or trigram analysis might miss). By recognizing such dependencies, the model can pick up on more complex complaints, such as food being cold, late service, or other nuanced grievances. This approach can be applied broadly to handle various review types without requiring specific human input each time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
